##刷Kaggle总结
1.数据exploration阶段是一个十分重要的阶段，在这个阶段你要选择模型的输入特征，因为有些题目它的原始数据可能会非常多，而且很多都是没有用的。譬如在Titanic或者Custom Statisfication都有一些无关的数据，你需要剔除无用的数据，保留有用的数据。Custom Satisfication有三百多列数据项，譬如里面的var2，你用DataFrame一看，离散的特别大，大部分都是999999，少部分在2,3之间晃荡，在讨论区很多人都认为这是国籍信息，但无论什么信息，我们可以把这一列数据当作常数列，如果是常数列，就可以删除，换句话说，就是无法提供信息，所带的信息非常少，好吧，这就跟信息论联系到一起了。
如果这一列数据与输出之间有一些比较明显的波动幅度，当然，对于连续型的数据，我们可以直接画出曲线，如下图所示的曲线，对于离散的数据，我们也可以画出bar图，看出某些数据的之间的差异。这里不得不说下pandas工具实在是非常好用，直接可以在后面加上plot，按照你的需求画图。
对于单个变量值的统计可以用DataFrame.var.value_count().plot()，还可以通过DataFrame.var1[var2=='val'].value.counts()进行每一类的统计。这种value_counts对离散的数值非常好用。对DataFrame的赋值更HASH一样。这样对每个特征进行选择。DataFrame取列也可以用(i)loc，取行就直接用行数。
接下来就说下除了matplotlib之外，另一个画图利器seaborn，seaborn调用格式统一，较容易记住。
另外好像还有一个图，显示了每个变量的方差，叫做boxplot。
2.特征与特征之间的关系，这也是Data Exploration的一部分。我们通常用DataFrame.var0[var1==val]得知某一值的分布，当var1=TARGET的时候，意义大家也清楚了。之所以要探索两个变量之间的关系，是因为有些变量的相关性非常大，大到可以只用一个其中一个变量就可以，我们就可以drop掉这一列数据了。
有个radviz画图工具，不知道咋回事。
DataFrame有个cor()函数，可以直接计算每个特征之间的互关系。
3.对特征的解释，我们常对有些数据感觉无法解释，但是通过Data Exploration，很明显有些数据对结果有明显的影响。我们对数据的一个很好的解释也是十分有意思的，就拿Titanic的数据来说，家人个数也会对获救率有所影响，因为这类人求生欲望更加强烈吧，或者更加有钱些吧。对于这种数据的理解，有时候可以将有些数据进行处理，譬如可以将家人数目和堂兄弟数目合在一起。同样，对Custom Satisfication中的var2也使得，加入解释成国籍代号，好像也没怎么重要。对特征的解释，也有利于baseline模型的选择。
4.特征选择技术，经过Data Exploration，你有了一个特征集合，但是仍旧可以作进一步的优化，就是特征选择，这些特征选择方法就很多了，这里就不具体解释了，另外一篇里面有相关的东西。
**5.对于文本特征的数字化**
我们会遇到很多的文本特征，譬如哪个港口上的船，哪个层次的cabin，性别，（我们有时候也需要将数字转化成离散的数字表示，很明显的例子就是年龄转化成儿童，少年，青年，成人和老人），在San Crime例子里面，我们还将DataFrame中的数字转化成年月日，等等。这里我们需要对DataFrame中的文本或者数字做单独的处理，再去掉响应的数据，再contact到总的DataFrame里面，我们这里主要总结下DataFrame，DataFrame.serise和数组之间如何转化。
DataFrame转化成数组比较直接，可以直接取出来就是数组了，
但是数组转化成DataFrame就需要将数组转化成serise，然后再用serise拼接到DataFrame，这种方法速度还是蛮慢的。当然也可以直接转化成矩阵，DataFrame[var=='val'].as_matrix，也可以用numpy.array进行强制转换。
DataFrame.iloc[]可以取出相关列，常用来提取特征和TARGET，也可以用DataFrame.var来提取具体某一行，但是很难做到整块提取。
6.模型的选择。一般的，特征的标签比较明显的，决策数是个很好的选择，为了符合决策树，我们也需要将连续的年龄等转化成几个阶段，譬如儿童，少年，成人，老年等。对于连续值，我们常需要观察下数据，我们有一个t-SNE观察多维数据。
7.数据的Normalized和scale，很明显，做Nomalized使得运算更容易收敛，让数据统一分布到0-1之间。scale让数据分布在[-1,1]之间，使得收敛速度更加块，更可能收敛。
8.对奇异值的处理，譬如有一列数据，个别的远远偏离大部分值很多，我们需要对他们做一个截断处理，阶段技术虽然非常简单，但是很有作用，让结果更加收敛和正常。在word2vector代码里面也用了截断技术来处理词频太高的词，使得结果更加合理，更容易收敛。
9.对缺省值的处理，没有比预测出合理的缺省值更好的了，然后用平均值，有些情况下考虑用最大值或者最小值。当然预测的方法也是一个分类或者拟合的问题了。
10.对模型的bagging。不可不说，bagging是提高预测精度的很好的方法，还是一个提高泛化能力的一个利器，因为子集的过拟合和全部集合的过拟合的关系。
11.Pipeline技术，Pipeline就是将很多的技术放到一块，形成一个流程，类似于ETL的功能，但是远比ETL简单。
12.模型的融合，就是用几个分类器分类数据，并且投票选出最好的模型，我们通常用数组存储模型，并用数组的子集来试验。
13.模型的cross validation，这个是必须用的了东西。但是有时候为了Early Stop，我们会循环到一定次数就会选择一个最优的结果的参数。这种方法常在训练神经网络中常用。另外一点就是常用随机话选取训练集和测试集，用random.shuffle(traingData)，然后再分数据为训练集和测试集。
14.另外一个就是一个小技巧，我们通常在写程序的时候，较大的数据会减慢速度，所以我们会抽取一部分数据进行试验。
15.最后，推荐一个很好的工具，就是xgboost，这工具是陈天奇大神写的，非常好，对离异值，imbalance都有所考虑，速度非常快，效果也不错。
16.寒小阳博客上存在两个问题：1.相关性上没有筛选，譬如性别男女上，只需保留一个就可以了，这个指标可以用相关性指标来查看，pandasn里面就有相关的cor计算，2.另外就是没有利用到特征选择的指标，导致有些指标的选择只是通过看图来看。譬如我们先对性别做一个相关性，然后再对性别做一个特征选择。同样，港口信息也一样，只有三个港口，所以只需要保留其中两个信息即可。