##情感分类
###原理
主要是模型融合，将三个模型融合起来，达到更高的效率：
一个模型是基于产生模型的语言模型，一个是基于句子的连续表示，最后一个是文档中的表示，这个表示是采用了TF-IDF表示的词袋模型，但是这个权重是经过重新权重的。
分别介绍这三个模型。
####三个模型的介绍
#####产生模型---基于RNNLM
计算一篇文章的likelihood，但是需要解决文档中未登录词的作用，有一些词可能只出现在其中一个训练集中，另外一个训练集中没有出现，这样在测试的时候就会有问题：测试集中包含了训练集中没有的单词，为了解决这个问题，在打分的时候，对每一个未登录词加上一个惩罚项。
N-gram会因为持续增加的上下文的字数导致了计算复杂和消耗巨大内存，另外还有就是特别稀疏。而RNN却可以给出比较有潜能，可以无限长的上下文的信息，避免了稀疏性。当然RNN也有梯度爆炸和梯度消失的缺陷，即便如此，RNNLM仍旧是最好的语言模型。
#####基于权重的N-GRAM线性分类
这是一个判别式模型。分类文章的比较常用的方法救市一篇文章的就是用TOP-N的TF-IDF方法，但是这个方法是基于词袋模型的，缺乏了单词之间的顺序信息，因为更好的方法是保留局部的单词顺序信息，那就是bag-of-n-gram。在这个模型中，我们采用了基于贝叶斯的SVM算法，计算了（从正类文章中抽出的平均词频和从负类文章中抽出的平均词频）的对数比，也就是一个词W在正类文章中的平均词频的对数除以该词W在负类文章中的平均词频的对数值。
这里还有一个LR分类器，该分类器的输入是一篇文章的对数比向量，
#####句子向量
其实embedding的思想还是抓住了共现，在一个词窗里面，有多少个词是相互出现的，通过这样调整意义相关的词，总是会有意义相似的词在一起的。
####模型融合
模型融合的关键是如何打分，我们这里采用的打分依旧是通过概率来评判，我们这里采用了几何的平均数打分的方法。
具体可以参见论文。那么打分的概率权重值如何决定呢？文章是通过暴力搜索的方法做到的。
###实现
####用RNNLM训练
#####语料介绍
语料是什么？根据文章的介绍，语聊分为positive集合和negtive集合，分k别训练获得对应的模型，那么如何用RNNLM训练，以及怎么用RNNLM所获得到的模型呢？
这里用到的语料是acllmdb，包含了train文件夹和test文件夹，另外一个lmdb.vocab是字典文件和imdbEr.txt文件，在README文件中，有相关的介绍。下面通过README文件，来介绍这些文件和文件夹。
总共有50000个评价文件，分成25k个训练集和25k个测试集，所有的正面评价和负面评价的数目都是相等的，还有50000个没有标记的文件，目的是为了无监督学习。分数小于等于4是负面评价，大于等于7的是正面评价，终极爱你的是中性评价。打开子文件夹，里面的文本文件分别放在neg和pos文件夹中，其中对应的单词分数放在labeledBow.feat文件中，第一个是该评价的极性打分，后面是单词的顺序号和该单词在字典中的索引号，每个train文件夹中，有25001个文本文件，分别是25000个评价文件和1个对所有的文件都normalized的总的文件。
为了防止训练文件过大，取出语料中的10000个评价，9800个训练集，200个测试集，对正负训练集一样都是10000个。
#####指令介绍
取好语料后，我们看看如何使用rnnlm指令，指定文件自然不用说，说的是-hidden, -direct-order, -class, -debug, -bptt, -bptt-block和-binary这几个option。rnnlm需要指定训练文件和验证文件，在rnnlm文件中，有一个例子，关于如何使用rnnlm训练语料，./rnnlm -train train -valid valid -rnnlm model -hidden 40 -rand-seed 1 -debug 2 -bptt 3 -class 200。
-rnnlm指定了模型文件名，-hidden指定了隐层神经元个数为40个，-debug说明了训练过程要在屏幕上打印出来，-bptt指定了BP训练时，利用了前两层的error，也就是复制了两个rnn隐层单元，为什么要指定呢？是因为BPTT算法一般都是指复制多层的BPTT单元用来求时间展开的error，但是现实中明显不可能，所以采用了截断的方法，这里指的截断多少个单元就是用-bptt指定的。-class 200是使用了200个classes加速训练过程，至于为什么用这技术加速，在作者的文章中有相关说明，在用RNNLM训练的瓶颈是输出单元和隐层单元个数，在作者的文章中，指定了每个词所属的类别，输出的那么就是类别的数目，而并不是整个字典的数目，这个想法最初来自于将所有的1gram单词放在一起，从而减少了训练和预测的复杂度，这里会涉及到两个分布，一个是分类的分布，一个是该分类中的单词的分布，这叫做因子化输出层，另一个加速的方法是压缩层，使用压缩层步进能够减少运算复杂度，还可以减少参数的数目，压缩层是放在隐层和输出层之间的一层，可以用sigmoid激活函数来压缩，一般的线性函数很难达到压缩的目的，有时候也会用压缩层放在输入层和隐层之间，进一步减少运算复杂度和参数个数，但是常用投影层来取代压缩层的名字。一旦训练完成了，我们习惯用./rnnlm -rnnlm model -test test 测试集来评估这次训练结果。
#####RNNLM的优点
时至今日，有很多的语言模型了，baseline语言模型自然不用说，还有最大熵语言模型，结构化语言模型，随机森林语言模型，等等。常常会遇到有人奇怪RNNLM语言模型有何好处，为何要用它，作者在一篇文章中作出了比较，说明了rnnlm具有较低的混淆度，而且对于一般模型的稀疏性和平滑性而言，具有更好的性能。其实语言模型的稀疏性是很多的语言模型的弊病，这下有了rnn语言模型，可以减轻这种稀疏性，取得较好的平滑性。
#####模型文件的解读

####NB-SVM介绍
先来说下，看完这篇文章，觉得救市MNB和SVM两个模型的融合，采用的是和表示打分。
#####原理介绍
从名字就可以看出来，这是将NB和SVM结合起来的算法，作者声称可以达到非常好的分类，包括情感分析和主题分类。用语言模型的bigram当作特征，进行输入。相对于长文本而言，短文本的特征更加具有区分性，在NB-SVM中，大致的方法是，用NB的计数的对数比作为特征值，这种选择特征值的方法具有非常好的性能，而且更加健壮，用作者的话来说，救市SVM with NB features就是NBSVM，用NB和SVM的一个插值作为结果，作者经过试验，觉得MNB和SVM插值的表现最好，而且在短文本方面表现也不错。现在从源代码的角度说明下这个算法的流程。
首先是tokenize程序，输入的是一个句子，返回的是这个句子关于n-gram的数组。假设是3-gram，那么先是每个单词单独成特征，然后是2-gram，最后是3-gram。第二个函数是建立词典操作，这里一个巧妙的操作就是利用了python的自带的collections类，从它继承过来，这样完成之后，就获得了文档集的所有的n-gram的特征。第三个函数是指处理文件，