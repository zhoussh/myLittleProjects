#参数的调优
##超参数的调优
数据预处理，特征工程，模型选择，超参数的优化，模型融合，分析评价，这些都是机器学习中一体的问题，特征工程出来之后，选择几个模型，用常见的几个指标评价该模型，如基于k-fold Cross Validation，确定几个模型之后，再对模型进行调优，然后再对模型进行融合做pipeline，达到更好的效果。这里主要说说对超参数的优化。这里多说一句，对于模型而言，设计一个好的损失函数也是一个非常重要的课题。
###什么是超参数
顾名思义，都是参数，属于大参数的一类，是用来区别系统与系统之间的关系，譬如参数集S<sub>k</sub>控制着系统K，而参数集S<sub>k</sub>又是受系统K-1的影响，那么系统K-1的参数S<sub>k-1</sub>就是系统k的参数。放在贝叶斯统计中，先验分布的参数是后验分布的超参数，这种放在LDA中非常好理解，文档的主题分布alpha，每个词所携带的主题分布beta，这是先验分布，统计了文档中每个词的个数之后，作出了贝叶斯方法之后，得到了文档的主题分布，那么alpha和beta就是文档主题的后验分布的超参数。超参数由于跟最终的输出有着很难解释和直观的可能性，因此我们如何得到超参数的值也是一个比较重要的课题。一般的方法如下：
###方法
####Grid Search网络搜索
#####原理
网格搜索应该是最传统，最常用的方法了。对于一个参数集，给定每个参数空间，然后给出一系列的评价指标，用cross validation训练和评价，最后确定每个参数。这里有个前提，就是每个参数之间相互不影响，而且对每个参数都有着界限。
#####以LIBSVM中为例
譬如对SVM而言，使用RBF，有两个参数需要调优，分别是正则化参数C和核参数gamma，因为这些参数本身都是可以连续的，我们就需要对其进行步长和范围的限制，为了加快搜索速度和扩大搜索范围，可以先粗粒度搜索，再细粒度搜索。值得注意的是，这种搜索可以并行，大大的节省了调优的时间。好了，Talk is cheap, show me the code时间到了。

####贝叶斯优化
#####原理

#####以LDA为例

####随机搜索


####基于梯度的优化


##初始值的选择
###线性查找


###