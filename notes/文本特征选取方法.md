#一些特征选取方法的理解
&emsp;一般的chi-square方法，信息增益的方法最适合用来做文本分析，本文介绍两种方法，本文介绍的方法用于提取出特征集合（一般用TF-IDF，LDA，n-gram等方法提取特征）之后，用这两种方法从这些特征中选择哪些特征会带来较好的性能，当然我们更有必要的理解各种公式的物理意义，对各种公式作出优化，确保哪种公式更加适合自己的工程需要。譬如在较短文本分类中，我们对低频词的重视程度就要有所提升，我们就要相应的修改公式，以适应我们的需求。
##chi-square方法
###原理及其数学理解
&emsp;假设有对文本集，有特征集:
$$T =\{t_{k}|k = 1,2,3 ... m\}$$
&emsp;分类集中第j类中的训练样本为c<sub>j</sub>，n<sub>c<sub>j</sub></sub>，其中j=1,2,...r.。那么类别c<sub>j</sub>与特征t<sub>k</sub>之间的相关性则可以用如下式表示：
$$\chi^2 = \frac{(AD-BC)^2}{(A+B)(C+D)}$$
&emsp;我们对公式中A，B，C，D定义如下表格，对于任意特征t<sub>k</sub>和任意类别c<sub>j</sub>，我们能得到如下表格：
项目|属于类别c<sub>j</sub>|不属于类别c<sub>j</sub>|总计
---|--------------------|---------------------|---
包含特征t<sub>k</sub>|A|B|A+B
不包含特征t<sub>k</sub>|C|D|C+D
&emsp;接下来，我们就会对各个特征t<sub>k</sub>计算相应的值chi-square，并从达到小排序，一般选取前p个特征。
###优化--低频词缺陷
&emsp;chi-square方法计算量少，但是对于低频词过于重视，忽略掉了词频的因素，很多研究者提出了针对词频优化的chi-square方法。对于**高频词**，**低频词**都有所考虑，一般有两种有关词频的优化方法：
####关于类内部词频的优化：

####关于类之间的词频优化：

##IG方法
&emsp;IG方法的在于理解背后的物理意义，我们先列出信息增益的公式来看看，信息增益公式如下，其中，特征t<sub>k</sub>，类别是c<sub>j</sub>,那么该特征的信息争议如下：
$$IG(t_{k}) = H(c_{j}) - H(c_{j}|t_{k})$$
我们可以这么理解，有特征t<sub>k</sub>和没有特征t<sub>k</sub>之间的熵区别，如果区别越大，那么所带来的信息越大，那么就可以用该特征当该分类c<sub>j</sub>的特征。对于H(c<sub>j</sub>)比较好计算，其公式如下：
$$H(c_{j}) = \sum_{j=0}^n{P(c_{j})log(P(c_{j}))}$$
而对于特征t<sub>k</sub>，其相对熵为：
$$H(c_{j}|t_{k})=P(t_{k})\sum_{j=0}^n{P(c_{j}|t_{k})log(P(c_{j}|t_{k}))} + P(\overline{t_{k}})\sum_{j=0}^n{P(c_{j}|\overline{t_{k}})log(P(c_{j}|\overline{t_{k}}))}$$
需要注意的是，信息增益的方法所用的是特征出现和不出现的两种情况，也就是有特征重合的缺点，所以对于出现在各个类别的特征并没有很好的选择的情况，这种情况可以结合其他的方法进行优化，譬如TF-IDF，chi-square都行，这两者方法在具体类别的特征识别率还是可以的。
##LR的L1正则化
L1的正则化的原理就不用多说了，就说说如何做了。